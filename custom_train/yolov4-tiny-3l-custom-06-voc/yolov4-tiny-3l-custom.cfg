[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=64
subdivisions=1
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 50020
policy=steps
steps=40000,45000
scales=.1,.1


[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=relu

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=relu

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu

[route]
layers=-1

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=relu

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=relu

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=relu

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu

[route]
layers=-1

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=relu

[route]
layers = -1,-2
 
##################### p3 64*64*256 (15)
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=relu

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu

[route]
layers=-1

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu

[route]
layers = -1,-2

##################### p4 32*32*256 (23)
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu

##################################
##################### p5 16*16*256 (27)
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=relu

[upsample]
stride=2

##################### p4_1 32*32*384 (30)
[route]
layers = -1, 23

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=relu

[upsample]
stride=2

##################### p3_f 64*64*384 (33)
[route]
layers = -1, 15

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=relu

[convolutional]
size=1
stride=1
pad=1
filters=75
activation=linear



[yolo]
mask = 0,1,2
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=20
num=9
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6

[route]
layers = -3

##################### p3_3 32*32*128 (38)
[maxpool] 
size=2
stride=2

##################### p4_f 32*32*512 (39)
[route]
layers = -1, 31, 23

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=relu

[convolutional]
size=1
stride=1
pad=1
filters=75
activation=linear



[yolo]
mask = 3,4,5
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=20
num=9
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6

[route]
layers = -3

##################### p4_3 16*16*256 (44)
[maxpool] 
size=2
stride=2

##################### p5_f 16*16*512 (45)
[route]
layers = -1, 27

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=relu

[convolutional]
size=1
stride=1
pad=1
filters=75
activation=linear



[yolo]
mask = 6,7,8
anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401
classes=20
num=9
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6
